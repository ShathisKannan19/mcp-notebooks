{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca72040d",
   "metadata": {},
   "source": [
    "## Creating a MCP Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de52ad0",
   "metadata": {},
   "source": [
    "### Building your own MCP\n",
    "#### This will run inside our HOST or Our own Application\n",
    "\n",
    "#### Reference Code\n",
    "``` python\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"uv\",  # Executable\n",
    "    args=[\"run example_server.py\"],  # Command line arguments\n",
    "    env=None,  # Optional environment variables\n",
    ")\n",
    "\n",
    "async def run():\n",
    "    # Launch the server as a subprocess & returns the read and write streams\n",
    "    # read: the stream that the client will use to read msgs from the server\n",
    "    # write: the stream that client will use to write msgs to the server\n",
    "    async with stdio_client(server_params) as (read, write): \n",
    "        # the client session is used to initiate the connection \n",
    "        # and send requests to server \n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection (1:1 connection with the server)\n",
    "            await session.initialize()\n",
    "\n",
    "            # List available tools\n",
    "            tools = await session.list_tools()\n",
    "\n",
    "            # will call the chat_loop here\n",
    "            # ....\n",
    "            \n",
    "            # Call a tool: this will be in the process_query method\n",
    "            result = await session.call_tool(\"tool-name\", arguments={\"arg1\": \"value\"})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run())\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b737ad",
   "metadata": {},
   "source": [
    "Here're the `mcp_chatbot` code `Anthropic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5f3164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/anthropic_mcp_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/anthropic_mcp_chatbot.py\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.session: ClientSession = None\n",
    "        self.anthropic = Anthropic()\n",
    "        self.available_tools: List[dict] = []\n",
    "\n",
    "    async def process_query(self, query):\n",
    "        messages = [{'role':'user', 'content':query}]\n",
    "        response = self.anthropic.messages.create(mapx_tokens = 2024,\n",
    "                                      model = 'claude-3-7-sonnet-20250219', \n",
    "                                      tools = self.available_tools, # tools exposed to the LLM\n",
    "                                      messages = messages)\n",
    "        process_query = True\n",
    "        while process_query:\n",
    "            assistant_content = []\n",
    "            for content in response.content:\n",
    "                if content.type =='text':\n",
    "                    print(content.text)\n",
    "                    assistant_content.append(content)\n",
    "                    if(len(response.content) == 1):\n",
    "                        process_query= False\n",
    "                elif content.type == 'tool_use':\n",
    "                    assistant_content.append(content)\n",
    "                    messages.append({'role':'assistant', 'content':assistant_content})\n",
    "                    tool_id = content.id\n",
    "                    tool_args = content.input\n",
    "                    tool_name = content.name\n",
    "    \n",
    "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                    \n",
    "                    # Call a tool\n",
    "                    #result = execute_tool(tool_name, tool_args): not anymore needed\n",
    "                    # tool invocation through the client session\n",
    "                    result = await self.session.call_tool(tool_name, arguments=tool_args)\n",
    "                    messages.append({\"role\": \"user\", \n",
    "                                      \"content\": [\n",
    "                                          {\n",
    "                                              \"type\": \"tool_result\",\n",
    "                                              \"tool_use_id\":tool_id,\n",
    "                                              \"content\": result.content\n",
    "                                          }\n",
    "                                      ]\n",
    "                                    })\n",
    "                    response = self.anthropic.messages.create(max_tokens = 2024,\n",
    "                                      model = 'claude-3-7-sonnet-20250219', \n",
    "                                      tools = self.available_tools,\n",
    "                                      messages = messages) \n",
    "                    \n",
    "                    if(len(response.content) == 1 and response.content[0].type == \"text\"):\n",
    "                        print(response.content[0].text)\n",
    "                        process_query= False\n",
    "\n",
    "    \n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "        \n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                    \n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "    \n",
    "    async def connect_to_server_and_run(self):\n",
    "        # Create server parameters for stdio connection\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"uv\",  # Executable\n",
    "            args=[\"run\", \"research_server.py\"],  # Optional command line arguments\n",
    "            env=None,  # Optional environment variables\n",
    "        )\n",
    "        async with stdio_client(server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                self.session = session\n",
    "                # Initialize the connection\n",
    "                await session.initialize()\n",
    "    \n",
    "                # List available tools\n",
    "                response = await session.list_tools()\n",
    "                \n",
    "                tools = response.tools\n",
    "                print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "                \n",
    "                self.available_tools = [{\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                } for tool in response.tools]\n",
    "    \n",
    "                await self.chat_loop()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    await chatbot.connect_to_server_and_run()\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955d21d",
   "metadata": {},
   "source": [
    "Here're the `mcp_chatbot` code `OpenAI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e5485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_project/openai_mcp_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/openai_mcp_chatbot.py\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.session: ClientSession = None\n",
    "        self.openai = OpenAI()\n",
    "        self.available_tools: List[dict] = []\n",
    "\n",
    "\n",
    "    async def process_query(self, query: str):\n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "        is_loop_nedded = True\n",
    "        while is_loop_nedded:\n",
    "            response = self.openai.responses.create(\n",
    "                model=\"gpt-4o\",\n",
    "                input=messages,\n",
    "                tools=self.available_tools,\n",
    "            )\n",
    "            for block in response.output:\n",
    "\n",
    "                if block.type == \"message\":\n",
    "                    is_loop_nedded = False\n",
    "                    return block.content[0].text\n",
    "\n",
    "                elif block.type == \"function_call\":\n",
    "                    tool_call_id = block.call_id\n",
    "                    tool_name = block.name\n",
    "                    tool_args = json.loads(block.arguments)\n",
    "\n",
    "                    print(\n",
    "                        f\"Processing tool call: {tool_name} with args: {tool_args} with call_id: {tool_call_id}\"\n",
    "                    )\n",
    "                    # Call a tool\n",
    "                    # result = execute_tool(tool_name, tool_args)\n",
    "                    # tool invocation through the client session\n",
    "                    result = await self.session.call_tool(tool_name, arguments=tool_args)\n",
    "                    messages.append(block.model_dump())\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"type\": \"function_call_output\",\n",
    "                            \"call_id\": tool_call_id,\n",
    "                            \"output\": str(result),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    is_loop_nedded = True\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery :[Type 'quit' to exit] \")\n",
    "                if not query.strip():\n",
    "                    print(\"Query cannot be empty. Please try again.\")\n",
    "                    continue\n",
    "\n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "\n",
    "                final_response = await self.process_query(query)\n",
    "                print(f\"Response : {final_response}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "    async def connect_to_server_and_run(self):\n",
    "        # Create server parameters for stdio connection\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"uv\",  # Executable\n",
    "            args=[\"run\", \"research_server.py\"],  # Optional command line arguments\n",
    "            env=None,  # Optional environment variables\n",
    "        )\n",
    "        async with stdio_client(server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                self.session = session\n",
    "                # Initialize the connection\n",
    "                await session.initialize()\n",
    "\n",
    "                # List available tools\n",
    "                response = await session.list_tools()\n",
    "\n",
    "                tools = response.tools\n",
    "                print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "\n",
    "                self.available_tools = [{\n",
    "                    \"type\": \"function\",\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"parameters\": tool.inputSchema,\n",
    "                } for tool in response.tools]\n",
    "\n",
    "                # Add the \"additionalProperties\": False : For OpenAI compatibility\n",
    "                for tool in self.available_tools:\n",
    "                    tool[\"parameters\"][\"additionalProperties\"] = False\n",
    "\n",
    "                await self.chat_loop()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    await chatbot.connect_to_server_and_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9be3d4",
   "metadata": {},
   "source": [
    "## Run the MCP Client and Server\n",
    "\n",
    "```bash\n",
    "# Already we have the Virtual Environment created\n",
    "cd mcp_project\n",
    "source .venv/bin/activate\n",
    "\n",
    "# Install the required packages\n",
    "uv add anthropic openai python-dotenv nest_asyncio\n",
    "\n",
    "# Terminal : Run the MCP Client\n",
    "uv run openai_mcp_chatbot.py # This is our End Working ChatBot Which is Interacting with the MCP Server for Tools\n",
    "\n",
    "# Finally you can Able to Ask the Question and get the tools response with the AI Model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43579ed",
   "metadata": {},
   "source": [
    "### This is the response from our chatbot using the MCP server tools to run and interact with AI models\n",
    "\n",
    "Here are some papers related to AI in Psychology for understanding and normalizing human emotions:\n",
    "\n",
    "1. **The Real Her? Exploring Whether Young Adults Accept Human-AI Love**\n",
    "   - **Authors**: Shuning Zhang, Shixuan Li\n",
    "   - **Summary**: This paper explores the acceptance of human-AI relationships among young adults, focusing on emotional interactions with AI companions and their implications.\n",
    "   - [Read more](http://arxiv.org/pdf/2503.03067v1)\n",
    "   - **Published**: 2025-03-05\n",
    "\n",
    "2. **The Good, The Bad, and Why: Unveiling Emotions in Generative AI**\n",
    "   - **Authors**: Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Xinyi Wang, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie\n",
    "   - **Summary**: This paper delves into how generative AI models comprehend emotions and employs approaches like EmotionPrompt and EmotionDecode for better AI performance.\n",
    "   - [Read more](http://arxiv.org/pdf/2312.11111v3)\n",
    "   - **Published**: 2023-12-18\n",
    "\n",
    "3. **AI shares emotion with humans across languages and cultures**\n",
    "   - **Authors**: Xiuwen Wu, Hao Wang, Zhiang Yan, Xiaohan Tang, Pengfei Xu, Wai-Ting Siok, Ping Li, Jia-Hong Gao, Bingjiang Lyu, Lang Qin\n",
    "   - **Summary**: This study investigates how AI systems align with human emotional perception across different languages and cultures, demonstrating AI's structural emotional congruence with humans.\n",
    "   - [Read more](http://arxiv.org/pdf/2506.13978v1)\n",
    "   - **Published**: 2025-06-11\n",
    "\n",
    "4. **Human-like Affective Cognition in Foundation Models**\n",
    "   - **Authors**: Kanishk Gandhi, Zoe Lynch, Jan-Philipp Fr√§nken, Kayla Patterson, Sharon Wambu, Tobias Gerstenberg, Desmond C. Ong, Noah D. Goodman\n",
    "   - **Summary**: This paper evaluates foundation models' abilities to mimic human-like emotional cognition, finding that they can often predict human emotional judgments accurately.\n",
    "   - [Read more](http://arxiv.org/pdf/2409.11733v2)\n",
    "   - **Published**: 2024-09-18\n",
    "\n",
    "5. **Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances**\n",
    "   - **Authors**: Soujanya Poria, Navonil Majumder, Rada Mihalcea, Eduard Hovy\n",
    "   - **Summary**: This paper discusses advances and challenges in emotion recognition in conversations, emphasizing the need for scalable algorithms for emotion-aware dialogues.\n",
    "   - [Read more](http://arxiv.org/pdf/1905.02947v1)\n",
    "   - **Published**: 2019-05-08\n",
    "\n",
    "These papers offer insights into how AI might understand and interact with human emotions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
