{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16761148",
   "metadata": {},
   "source": [
    "# Adding Prompt & Resource Features\n",
    "\n",
    "As we have already playaround the MCP Server and Client, To interact with multiple tools with different servers to access the data or some functionality, we can use it within the MCP Client.\n",
    "Here in this below example, We are implementing a MCP Server with a resources and prompts to access the any data[arvix data] and then using the MCP Client to interact with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0b161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/research_server_with_resource.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/research_server_with_resource.py\n",
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "\n",
    "PAPER_DIR = \"papers\"\n",
    "\n",
    "# Initialize FastMCP server\n",
    "mcp = FastMCP(\"research\")\n",
    "\n",
    "@mcp.tool()\n",
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for papers on arXiv based on a topic and store their information.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "        max_results: Maximum number of results to retrieve (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of paper IDs found in the search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use arxiv to find the papers \n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the most relevant articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query = topic,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "    \n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    # Process each paper and add to papers_info  \n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id())\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date())\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "    \n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "    \n",
    "    print(f\"Results are saved in: {file_path}\")\n",
    "    \n",
    "    return paper_ids\n",
    "\n",
    "@mcp.tool()\n",
    "def extract_info(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information about a specific paper across all topic directories.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The ID of the paper to look for\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with paper information if found, error message if not found\n",
    "    \"\"\"\n",
    " \n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    return f\"There's no saved information related to paper {paper_id}.\"\n",
    "\n",
    "# Above tools are already we see in the MCP server, so we can use them directly. and Below we need to add the resource to the MCP server\n",
    "\n",
    "# RESOURCES\n",
    "@mcp.resource(\"papers://folders\")\n",
    "def get_available_folders() -> str:\n",
    "    \"\"\"\n",
    "    List all available topic folders in the papers directory.\n",
    "    \n",
    "    This resource provides a simple list of all available topic folders.\n",
    "    \"\"\"\n",
    "    folders = []\n",
    "    \n",
    "    # Get all topic directories\n",
    "    if os.path.exists(PAPER_DIR):\n",
    "        for topic_dir in os.listdir(PAPER_DIR):\n",
    "            topic_path = os.path.join(PAPER_DIR, topic_dir)\n",
    "            if os.path.isdir(topic_path):\n",
    "                papers_file = os.path.join(topic_path, \"papers_info.json\")\n",
    "                if os.path.exists(papers_file):\n",
    "                    folders.append(topic_dir)\n",
    "    \n",
    "    # Create a simple markdown list\n",
    "    content = \"# Available Topics\\n\\n\"\n",
    "    if folders:\n",
    "        for folder in folders:\n",
    "            content += f\"- {folder}\\n\"\n",
    "        content += f\"\\nUse @{folder} to access papers in that topic.\\n\"\n",
    "    else:\n",
    "        content += \"No topics found.\\n\"\n",
    "    \n",
    "    return content\n",
    "\n",
    "@mcp.resource(\"papers://{topic}\")\n",
    "def get_topic_papers(topic: str) -> str:\n",
    "    \"\"\"\n",
    "    Get detailed information about papers on a specific topic.\n",
    "    \n",
    "    Args:\n",
    "        topic: The research topic to retrieve papers for\n",
    "    \"\"\"\n",
    "    topic_dir = topic.lower().replace(\" \", \"_\")\n",
    "    papers_file = os.path.join(PAPER_DIR, topic_dir, \"papers_info.json\")\n",
    "    \n",
    "    if not os.path.exists(papers_file):\n",
    "        return f\"# No papers found for topic: {topic}\\n\\nTry searching for papers on this topic first.\"\n",
    "    \n",
    "    try:\n",
    "        with open(papers_file, 'r') as f:\n",
    "            papers_data = json.load(f)\n",
    "        \n",
    "        # Create markdown content with paper details\n",
    "        content = f\"# Papers on {topic.replace('_', ' ').title()}\\n\\n\"\n",
    "        content += f\"Total papers: {len(papers_data)}\\n\\n\"\n",
    "        \n",
    "        for paper_id, paper_info in papers_data.items():\n",
    "            content += f\"## {paper_info['title']}\\n\"\n",
    "            content += f\"- **Paper ID**: {paper_id}\\n\"\n",
    "            content += f\"- **Authors**: {', '.join(paper_info['authors'])}\\n\"\n",
    "            content += f\"- **Published**: {paper_info['published']}\\n\"\n",
    "            content += f\"- **PDF URL**: [{paper_info['pdf_url']}]({paper_info['pdf_url']})\\n\\n\"\n",
    "            content += f\"### Summary\\n{paper_info['summary'][:500]}...\\n\\n\"\n",
    "            content += \"---\\n\\n\"\n",
    "        \n",
    "        return content\n",
    "    except json.JSONDecodeError:\n",
    "        return f\"# Error reading papers data for {topic}\\n\\nThe papers data file is corrupted.\"\n",
    "\n",
    "# PROMPTS [With Dynamic Informations we pass as Arguments]\n",
    "@mcp.prompt()\n",
    "def generate_search_prompt(topic: str, num_papers: int = 5) -> str:\n",
    "    \"\"\"Generate a prompt for Claude to find and discuss academic papers on a specific topic.\"\"\"\n",
    "    return f\"\"\"Search for {num_papers} academic papers about '{topic}' using the search_papers tool. Follow these instructions:\n",
    "    1. First, search for papers using search_papers(topic='{topic}', max_results={num_papers})\n",
    "    2. For each paper found, extract and organize the following information:\n",
    "       - Paper title\n",
    "       - Authors\n",
    "       - Publication date\n",
    "       - Brief summary of the key findings\n",
    "       - Main contributions or innovations\n",
    "       - Methodologies used\n",
    "       - Relevance to the topic '{topic}'\n",
    "    \n",
    "    3. Provide a comprehensive summary that includes:\n",
    "       - Overview of the current state of research in '{topic}'\n",
    "       - Common themes and trends across the papers\n",
    "       - Key research gaps or areas for future investigation\n",
    "       - Most impactful or influential papers in this area\n",
    "    \n",
    "    4. Organize your findings in a clear, structured format with headings and bullet points for easy readability.\n",
    "    \n",
    "    Please present both detailed information about each paper and a high-level synthesis of the research landscape in {topic}.\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run the server\n",
    "    mcp.run(transport='stdio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6fe1055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/openai_chatbot_with_resource_and_prompts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/openai_chatbot_with_resource_and_prompts.py\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import TypedDict\n",
    "from contextlib import AsyncExitStack\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    parameters: dict\n",
    "    type: str = \"function\"\n",
    "\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.openai = OpenAI()\n",
    "        self.available_tools = []\n",
    "        self.available_prompts= []\n",
    "        self.sessions= {}\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_config: dict) -> None:\n",
    "        \"\"\"Connect to a single MCP server.\"\"\"\n",
    "        try:\n",
    "            server_params = StdioServerParameters(**server_config)\n",
    "            stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            )\n",
    "            read, write = stdio_transport\n",
    "            session = await self.exit_stack.enter_async_context(\n",
    "                ClientSession(read, write)\n",
    "            )\n",
    "            await session.initialize()\n",
    "\n",
    "            try:\n",
    "                # List available tools for this session\n",
    "                response = await session.list_tools()\n",
    "                tools = response.tools\n",
    "                print(f\"\\nConnected to {server_name} with tools:\", [t.name for t in tools])\n",
    "                for tool in tools:\n",
    "                    self.sessions[tool.name] = session\n",
    "                    self.available_tools.append(\n",
    "                        {\n",
    "                            \"type\": \"function\",\n",
    "                            \"name\": tool.name,\n",
    "                            \"description\": tool.description,\n",
    "                            \"parameters\": tool.inputSchema,\n",
    "                        }\n",
    "                    )\n",
    "                \n",
    "                for tool in self.available_tools:\n",
    "                    tool[\"parameters\"][\"additionalProperties\"] = False\n",
    "\n",
    "                prompts_response = await session.list_prompts()\n",
    "                if prompts_response and prompts_response.prompts:\n",
    "                    print(f\"\\nConnected to {server_name} with prompts:\", [p.name for p in prompts_response.prompts])\n",
    "                    for prompt in prompts_response.prompts:\n",
    "                        self.sessions[prompt.name] = session\n",
    "                        self.available_prompts.append({\n",
    "                            \"name\": prompt.name,\n",
    "                            \"description\": prompt.description,\n",
    "                            \"arguments\": prompt.arguments\n",
    "                        })\n",
    "                    \n",
    "\n",
    "                # List Available resources for this session\n",
    "                resources_response = await session.list_resources()\n",
    "                if resources_response and resources_response.resources:\n",
    "                    print(f\"\\nConnected to {server_name} with resources:\", [str(r.uri) for r in resources_response.resources])\n",
    "                    for resource in resources_response.resources:\n",
    "                        resource_uri = str(resource.uri)\n",
    "                        self.sessions[resource_uri] = session\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self):  # new\n",
    "        \"\"\"Connect to all configured MCP servers.\"\"\"\n",
    "        try:\n",
    "            with open(\"server_config_with_prompts_and_resource.json\", \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            servers = data.get(\"mcpServers\", {})\n",
    "\n",
    "            for server_name, server_config in servers.items():\n",
    "                await self.connect_to_server(server_name, server_config)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading server configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def process_query(self, query: str):\n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "        is_loop_nedded = True\n",
    "        while is_loop_nedded:\n",
    "            response = self.openai.responses.create(\n",
    "                model=\"gpt-4o\",\n",
    "                input=messages,\n",
    "                tools=self.available_tools,\n",
    "            )\n",
    "            for block in response.output:\n",
    "                if block.type == \"message\":\n",
    "                    is_loop_nedded = False\n",
    "                    print(block.content[0].text)\n",
    "\n",
    "                elif block.type == \"function_call\":\n",
    "                    tool_call_id = block.call_id\n",
    "                    tool_name = block.name\n",
    "                    tool_args = json.loads(block.arguments)\n",
    "\n",
    "                    print(\n",
    "                        f\"Processing tool call: {tool_name} with args: {tool_args} with call_id: {tool_call_id}\"\n",
    "                    )\n",
    "\n",
    "                    # Call a tool\n",
    "                    session = self.sessions.get(block.name)\n",
    "                    result = await session.call_tool(tool_name, arguments=tool_args)\n",
    "                    messages.append(block.model_dump())\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"type\": \"function_call_output\",\n",
    "                            \"call_id\": tool_call_id,\n",
    "                            \"output\": str(result),\n",
    "                        }\n",
    "                    )\n",
    "                    is_loop_nedded = True\n",
    "\n",
    "\n",
    "    async def get_resource(self, resource_uri):\n",
    "        session = self.sessions.get(resource_uri)\n",
    "\n",
    "        # Fallback for papers URIs - try any papers resource session\n",
    "        if not session and resource_uri.startswith(\"papers://\"):\n",
    "            for uri, sess in self.sessions.items():\n",
    "                if uri.startswith(\"papers://\"):\n",
    "                    session = sess\n",
    "                    break\n",
    "\n",
    "        if not session:\n",
    "            print(f\"Resource '{resource_uri}' not found.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            result = await session.read_resource(uri=resource_uri)\n",
    "            if result and result.contents:\n",
    "                print(f\"\\nResource: {resource_uri}\")\n",
    "                print(\"Content:\")\n",
    "                print(result.contents[0].text)\n",
    "            else:\n",
    "                print(\"No content available.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    async def list_prompts(self):\n",
    "        \"\"\"List all available prompts.\"\"\"\n",
    "        if not self.available_prompts:\n",
    "            print(\"No prompts available.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\nAvailable prompts:\")\n",
    "        for prompt in self.available_prompts:\n",
    "            print(f\"- {prompt['name']}: {prompt['description']}\")\n",
    "            if prompt['arguments']:\n",
    "                print(\"  Arguments:\")\n",
    "                for arg in prompt['arguments']:\n",
    "                    arg_name = arg.name if hasattr(arg, 'name') else arg.get('name', '')\n",
    "                    print(f\"    - {arg_name}\")\n",
    "\n",
    "    async def execute_prompt(self, prompt_name, args):\n",
    "        \"\"\"Execute a prompt with the given arguments.\"\"\"\n",
    "        session = self.sessions.get(prompt_name)\n",
    "        if not session:\n",
    "            print(f\"Prompt '{prompt_name}' not found.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            result = await session.get_prompt(prompt_name, arguments=args)\n",
    "            if result and result.messages:\n",
    "                prompt_content = result.messages[0].content\n",
    "\n",
    "                # Extract text from content (handles different formats)\n",
    "                if isinstance(prompt_content, str):\n",
    "                    text = prompt_content\n",
    "                elif hasattr(prompt_content, 'text'):\n",
    "                    text = prompt_content.text\n",
    "                else:\n",
    "                    # Handle list of content items\n",
    "                    text = \" \".join(item.text if hasattr(item, 'text') else str(item) \n",
    "                                  for item in prompt_content)\n",
    "\n",
    "                print(f\"\\nExecuting prompt '{prompt_name}'...\")\n",
    "                await self.process_query(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        print(\"Use @folders to see available topics\")\n",
    "        print(\"Use @<topic> to search papers in that topic\")\n",
    "        print(\"Use /prompts to list available prompts\")\n",
    "        print(\"Use /prompt <name> <arg1=value1> to execute a prompt\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery :[Type 'quit' to exit]: \").strip()\n",
    "\n",
    "                if query.lower() == \"quit\":\n",
    "                    break\n",
    "\n",
    "                # Check for @resource syntax first\n",
    "                if query.startswith('@'):\n",
    "                    # Remove @ sign  \n",
    "                    topic = query[1:]\n",
    "                    if topic == \"folders\":\n",
    "                        resource_uri = \"papers://folders\"\n",
    "                    else:\n",
    "                        resource_uri = f\"papers://{topic}\"\n",
    "                    await self.get_resource(resource_uri)\n",
    "                    continue\n",
    "\n",
    "                # Check for /command syntax\n",
    "                if query.startswith('/'):\n",
    "                    parts = query.split()\n",
    "                    command = parts[0].lower()\n",
    "\n",
    "                    if command == '/prompts':\n",
    "                        await self.list_prompts()\n",
    "                    elif command == '/prompt':\n",
    "                        if len(parts) < 2:\n",
    "                            print(\"Usage: /prompt <name> <arg1=value1> <arg2=value2>\")\n",
    "                            continue\n",
    "\n",
    "                        prompt_name = parts[1]\n",
    "                        args = {}\n",
    "\n",
    "                        # Parse arguments\n",
    "                        for arg in parts[2:]:\n",
    "                            if '=' in arg:\n",
    "                                key, value = arg.split('=', 1)\n",
    "                                args[key] = value\n",
    "\n",
    "                        await self.execute_prompt(prompt_name, args)\n",
    "                    else:\n",
    "                        print(f\"Unknown command: {command}\")\n",
    "                    continue\n",
    "\n",
    "                await self.process_query(query)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "    async def cleanup(self):  # new\n",
    "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        # the mcp clients and sessions are not initialized using \"with\"\n",
    "        # like in the previous lesson\n",
    "        # so the cleanup should be manually handled\n",
    "        await chatbot.connect_to_servers()  # new!\n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup()  # new!\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28767618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/server_config_with_prompts_and_resource.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/server_config_with_prompts_and_resource.json\n",
    "{\n",
    "    \"mcpServers\": {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-filesystem\",\n",
    "                \".\"\n",
    "            ]\n",
    "        },\n",
    "        \"research\": {\n",
    "            \"command\": \"uv\",\n",
    "            \"args\": [\n",
    "                \"run\",\n",
    "                \"research_server_with_resource.py\"\n",
    "            ]\n",
    "        },\n",
    "        \"fetch\": {\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-fetch\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d17577b",
   "metadata": {},
   "source": [
    "### How to run and Explore the Prompts and Resources\n",
    "\n",
    "Step 1: Run the UV Virtual Environment to run MCP Client and Server with below steps\n",
    "\n",
    "```bash\n",
    "cd mcp_project # Before did this in your terminal, You need to go to our Repo's Root Directory\n",
    "\n",
    "source .venv/bin/activate # Activate the virtual environment\n",
    "```\n",
    "\n",
    "Step 2: Run the MCP Server with below command\n",
    "\n",
    "```bash\n",
    "uv run openai_chatbot_with_resource_and_prompts.py\n",
    "```\n",
    "\n",
    "Now, play around with the MCP Client using the following command and review the example screenshots below:\n",
    "\n",
    "After running the MCP Client, you can able to see the tools, propmts and resources available to use. from your own MCP Server and Also other references.\n",
    "<img src=\"assets/mcp_execution_using_resource_and_prompts_1.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">\n",
    "\n",
    "You can able to get the resources list using below command:\n",
    "\n",
    "Query : `@folders`\n",
    "\n",
    "<img src=\"assets/mcp_execution_using_resource_and_prompts_2.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">\n",
    "\n",
    "And you can Able to get the resources using the resource name with this format.\n",
    "\n",
    "Query : `@{resource_name}` - Here we able to get the entire information with in the `resource_name` folder. Example: `@@ai_agents`\n",
    "<img src=\"assets/mcp_execution_using_resource_and_prompts_3.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">\n",
    "\n",
    "Below example is to list the available prompts in the MCP Server, which is used to interact with the MCP Server and get the data from the resources.\n",
    "\n",
    "Query : `/prompts`\n",
    "\n",
    "<img src=\"assets/mcp_execution_using_resource_and_prompts_4.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">\n",
    "\n",
    "Below one is to use the prompt to interact with the tools and resources available in the MCP Server.\n",
    "\n",
    "Query : `/prompt`\n",
    "Response : `Usage: /prompt <name> <arg1=value1> <arg2=value2>`\n",
    "\n",
    "Query Example for using the prompt:\n",
    "\n",
    "```bash\n",
    "/prompt generate_search_prompt topic=love limit=5\n",
    "```\n",
    "\n",
    "<img src=\"assets/mcp_execution_using_resource_and_prompts_5.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb01b2",
   "metadata": {},
   "source": [
    "### How to Run and Explore the Prompts and Resources\n",
    "\n",
    "**Step 1:** Run the UV Virtual Environment to execute the MCP Client and Server with the steps below:\n",
    "\n",
    "```bash\n",
    "cd mcp_project # Before running this command, make sure you are in the root directory of the repository\n",
    "\n",
    "source .venv/bin/activate # Activate the virtual environment\n",
    "```\n",
    "\n",
    "**Step 2:** Run the MCP Server with the following command:\n",
    "\n",
    "```bash\n",
    "uv run openai_chatbot_with_resource_and_prompts.py\n",
    "```\n",
    "\n",
    "Now, you can experiment with the MCP Client using the following command and review the example screenshots below:\n",
    "\n",
    "After running the MCP Client, you will be able to see the tools, prompts, and resources available from your own MCP Server as well as other references. <img src=\"assets/mcp_execution_using_resource_and_prompts_1.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">\n",
    "\n",
    "You can retrieve the list of resources using the following command:\n",
    "\n",
    "**Query:** `@folders`\n",
    "\n",
    "<img src=\"assets/mcp_execution_using_resource_and_prompts_2.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">\n",
    "\n",
    "You can also retrieve resources by using the resource name in the following format:\n",
    "\n",
    "**Query:** `@{resource_name}` â€” This will return all the information within the specified `resource_name` folder.\n",
    "\n",
    "Example: `@ai_agents` <img src=\"assets/mcp_execution_using_resource_and_prompts_3.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">\n",
    "\n",
    "The example below lists all the available prompts in the MCP Server, which you can use to interact with the server and retrieve data from the resources.\n",
    "\n",
    "**Query:** `/prompts`\n",
    "\n",
    "<img src=\"assets/mcp_execution_using_resource_and_prompts_4.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">\n",
    "\n",
    "The following query demonstrates how to use a prompt to interact with the available tools and resources in the MCP Server.\n",
    "\n",
    "**Query:** `/prompt`\n",
    "\n",
    "**Response:**\n",
    "\n",
    "```\n",
    "Usage: /prompt <name> <arg1=value1> <arg2=value2>\n",
    "```\n",
    "\n",
    "Example query for using a prompt:\n",
    "\n",
    "```bash\n",
    "/prompt generate_search_prompt topic=love limit=5\n",
    "```\n",
    "\n",
    "<img src=\"assets/mcp_execution_using_resource_and_prompts_5.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">\n",
    "\n",
    "> NOTE: Hereâ€™s the wrap-up in a short, simple points format:\n",
    "\n",
    "- We ran the _MCP Server and MCP Client_\n",
    "- We explored _tools_ available in the MCP Server\n",
    "- We listed and _fetched resources_\n",
    "- We viewed all _prompts_\n",
    "- We used _prompts with arguments_ to get results\n",
    "\n",
    "Now you know how to use **tools, prompts, and resources** via the _MCP Client and Server_! Tada! ðŸŽ‰\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
