{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b741342f",
   "metadata": {},
   "source": [
    "# Connecting the MCP Chatbot to Reference Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaffa5e4",
   "metadata": {},
   "source": [
    "#### Chatbot Implementation\n",
    "\n",
    "- Anthropic MCP Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3cc5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_project/anthropic_mcp_chatbot_reference_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/anthropic_mcp_chatbot_reference_server.py\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict\n",
    "from contextlib import AsyncExitStack\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.sessions: List[ClientSession] = []  # new\n",
    "        self.exit_stack = AsyncExitStack()  # new\n",
    "        self.anthropic = Anthropic()\n",
    "        self.available_tools: List[ToolDefinition] = []  # new\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {}  # new\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_config: dict) -> None:\n",
    "        \"\"\"Connect to a single MCP server.\"\"\"\n",
    "        try:\n",
    "            server_params = StdioServerParameters(**server_config)\n",
    "            stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            )  # new\n",
    "            read, write = stdio_transport\n",
    "            session = await self.exit_stack.enter_async_context(\n",
    "                ClientSession(read, write)\n",
    "            )  # new\n",
    "            await session.initialize()\n",
    "            self.sessions.append(session)\n",
    "\n",
    "            # List available tools for this session\n",
    "            response = await session.list_tools()\n",
    "            tools = response.tools\n",
    "            print(f\"\\nConnected to {server_name} with tools:\", [t.name for t in tools])\n",
    "\n",
    "            for tool in tools:  # new\n",
    "                self.tool_to_session[tool.name] = session\n",
    "                self.available_tools.append(\n",
    "                    {\n",
    "                        \"name\": tool.name,\n",
    "                        \"description\": tool.description,\n",
    "                        \"input_schema\": tool.inputSchema,\n",
    "                    }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self):  # new\n",
    "        \"\"\"Connect to all configured MCP servers.\"\"\"\n",
    "        try:\n",
    "            with open(\"server_config.json\", \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            servers = data.get(\"mcpServers\", {})\n",
    "\n",
    "            for server_name, server_config in servers.items():\n",
    "                await self.connect_to_server(server_name, server_config)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading server configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def process_query(self, query):\n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        response = self.anthropic.messages.create(\n",
    "            max_tokens=2024,\n",
    "            model=\"claude-3-7-sonnet-20250219\",\n",
    "            tools=self.available_tools,\n",
    "            messages=messages,\n",
    "        )\n",
    "        process_query = True\n",
    "        while process_query:\n",
    "            assistant_content = []\n",
    "            for content in response.content:\n",
    "                if content.type == \"text\":\n",
    "                    print(content.text)\n",
    "                    assistant_content.append(content)\n",
    "                    if len(response.content) == 1:\n",
    "                        process_query = False\n",
    "                elif content.type == \"tool_use\":\n",
    "                    assistant_content.append(content)\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "                    tool_id = content.id\n",
    "                    tool_args = content.input\n",
    "                    tool_name = content.name\n",
    "\n",
    "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "\n",
    "                    # Call a tool\n",
    "                    session = self.tool_to_session[tool_name]  # new\n",
    "                    result = await session.call_tool(tool_name, arguments=tool_args)\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\": \"tool_result\",\n",
    "                                    \"tool_use_id\": tool_id,\n",
    "                                    \"content\": result.content,\n",
    "                                }\n",
    "                            ],\n",
    "                        }\n",
    "                    )\n",
    "                    response = self.anthropic.messages.create(\n",
    "                        max_tokens=2024,\n",
    "                        model=\"claude-3-7-sonnet-20250219\",\n",
    "                        tools=self.available_tools,\n",
    "                        messages=messages,\n",
    "                    )\n",
    "\n",
    "                    if (\n",
    "                        len(response.content) == 1\n",
    "                        and response.content[0].type == \"text\"\n",
    "                    ):\n",
    "                        print(response.content[0].text)\n",
    "                        process_query = False\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "\n",
    "                if query.lower() == \"quit\":\n",
    "                    break\n",
    "\n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "    async def cleanup(self):  # new\n",
    "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        # the mcp clients and sessions are not initialized using \"with\"\n",
    "        # like in the previous lesson\n",
    "        # so the cleanup should be manually handled\n",
    "        await chatbot.connect_to_servers()  # new!\n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup()  # new!\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425ce44",
   "metadata": {},
   "source": [
    "- OpenAI MCP Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e88a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/openai_mcp_chatbot_reference_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/openai_mcp_chatbot_reference_server.py\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict\n",
    "from contextlib import AsyncExitStack\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.sessions: List[ClientSession] = []  # new\n",
    "        self.exit_stack = AsyncExitStack()  # new\n",
    "        self.openai = OpenAI()\n",
    "        self.available_tools: List[ToolDefinition] = []  # new\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {}  # new\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_config: dict) -> None:\n",
    "        \"\"\"Connect to a single MCP server.\"\"\"\n",
    "        try:\n",
    "            server_params = StdioServerParameters(**server_config)\n",
    "            stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            )  # new\n",
    "            read, write = stdio_transport\n",
    "            session = await self.exit_stack.enter_async_context(\n",
    "                ClientSession(read, write)\n",
    "            )  # new\n",
    "            await session.initialize()\n",
    "            self.sessions.append(session)\n",
    "\n",
    "            # List available tools for this session\n",
    "            response = await session.list_tools()\n",
    "            tools = response.tools\n",
    "            print(f\"\\nConnected to {server_name} with tools:\", [t.name for t in tools])\n",
    "\n",
    "            for tool in tools:  # new\n",
    "                self.tool_to_session[tool.name] = session\n",
    "                self.available_tools.append(\n",
    "                    {\n",
    "                        \"type\": \"function\",\n",
    "                        \"name\": tool.name,\n",
    "                        \"description\": tool.description,\n",
    "                        \"parameters\": tool.inputSchema,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                for tool in self.available_tools: \n",
    "                    tool[\"parameters\"][\"additionalProperties\"] = False\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self):  # new\n",
    "        \"\"\"Connect to all configured MCP servers.\"\"\"\n",
    "        try:\n",
    "            with open(\"server_config.json\", \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            servers = data.get(\"mcpServers\", {})\n",
    "\n",
    "            for server_name, server_config in servers.items():\n",
    "                await self.connect_to_server(server_name, server_config)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading server configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def process_query(self, query: str):\n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "        is_loop_nedded = True\n",
    "        while is_loop_nedded:\n",
    "            response = self.openai.responses.create(\n",
    "                model=\"gpt-4o\",\n",
    "                input=messages,\n",
    "                tools=self.available_tools,\n",
    "            )\n",
    "            for block in response.output:\n",
    "                if block.type == \"message\":\n",
    "                    is_loop_nedded = False\n",
    "                    return block.content[0].text\n",
    "\n",
    "                elif block.type == \"function_call\":\n",
    "                    tool_call_id = block.call_id\n",
    "                    tool_name = block.name\n",
    "                    tool_args = json.loads(block.arguments)\n",
    "\n",
    "                    print(\n",
    "                        f\"Processing tool call: {tool_name} with args: {tool_args} with call_id: {tool_call_id}\"\n",
    "                    )\n",
    "\n",
    "                    # Call a tool\n",
    "                    session = self.tool_to_session[tool_name]  # new\n",
    "                    result = await session.call_tool(tool_name, arguments=tool_args)\n",
    "                    messages.append(block.model_dump())\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"type\": \"function_call_output\",\n",
    "                            \"call_id\": tool_call_id,\n",
    "                            \"output\": str(result),\n",
    "                        }\n",
    "                    )\n",
    "                    is_loop_nedded = True\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery :[Type 'quit' to exit]: \").strip()\n",
    "\n",
    "                if query.lower() == \"quit\":\n",
    "                    break\n",
    "\n",
    "                answer = await self.process_query(query)\n",
    "                print(\"{}\\n\". format(answer))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "    async def cleanup(self):  # new\n",
    "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        # the mcp clients and sessions are not initialized using \"with\"\n",
    "        # like in the previous lesson\n",
    "        # so the cleanup should be manually handled\n",
    "        await chatbot.connect_to_servers()  # new!\n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup()  # new!\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4bdb2b",
   "metadata": {},
   "source": [
    "# To Invoke the MCP Servers as Reference Servers\n",
    "\n",
    "Below we have mentioned the 3 reference servers that we will be connecting to:\n",
    "- **filesystems**: This server provides access to various file systems (i.e., local) for reading and writing files.\n",
    "- **research_papers**: This server allows querying and retrieving research papers.\n",
    "- **fetch**: This server is used to fetch data from external sources like APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856ac6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/server_config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/server_config.json\n",
    "{\n",
    "    \"mcpServers\": {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-filesystem\",\n",
    "                \".\"\n",
    "            ]\n",
    "        },\n",
    "        \"research\": {\n",
    "            \"command\": \"uv\",\n",
    "            \"args\": [\n",
    "                \"run\",\n",
    "                \"research_server.py\"\n",
    "            ]\n",
    "        },\n",
    "        \"fetch\": {\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-fetch\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f25cad",
   "metadata": {},
   "source": [
    "### How to Run the MCP Chatbot using the Reference Servers\n",
    "\n",
    "Steps to run the MCP Chatbot using the reference servers:\n",
    "1. **OpenYour Terminal**: Open your terminal or command prompt.\n",
    "2. **Navigate to the Directory**: Change to the directory where your MCP notebook is located as we stored within the `mcp_project` folder.\n",
    "   ```bash\n",
    "   cd /path/to/mcp-notebooks/cookbooks/mcp_project\n",
    "   ```\n",
    "3. **Create a Virtual Environments using UV**: \n",
    "   ```bash\n",
    "   uv init\n",
    "   ```\n",
    "4. **Install Required Packages**: Install the required packages using the following command:\n",
    "   ```bash\n",
    "   uv add anthropic mcp fastmcp openai arxiv python-dotenv nest-asyncio\n",
    "   ```\n",
    "\n",
    "5. **Run the MCP Notebook**: Start the MCP notebook using the following command:\n",
    "   ```bash\n",
    "   uv run openai_mcp_chatbot_reference_server.py\n",
    "   ```\n",
    "\n",
    "Finally, you can interact with the MCP Chatbot in your terminal. You can ask questions, and the chatbot will respond using the connected reference servers.\n",
    "\n",
    "#### The Terminal Output\n",
    "\n",
    "<img src=\"assets/mcp_references_server_connect_via_mcp_client_chatbot.png\" width=\"auto\" alt=\"MCP Inspector - Search Papers Tools\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962b64d",
   "metadata": {},
   "source": [
    "#### The Output of the MCP Chatbot for The MCP Mermaid Diagram using Code\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[MCP Client] -->|Communicates with| B[MCP Server];\n",
    "    A --> C[User Interface];\n",
    "    A --> D[Local Database];\n",
    "    B --> E[Core Logic];\n",
    "    B --> F[External API];\n",
    "    E --> G[Data Processing];\n",
    "    E --> H[Business Logic];\n",
    "    F --> I[Third-party Service];\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
